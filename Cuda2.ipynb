{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-A0aaZqpKpH",
        "outputId": "526938b0-3141-4362-a440-37a4c4223a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting MatrixM.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile MatrixM.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "#define WIDTH 16  // Assuming a square matrix of WIDTH x WIDTH 1024\n",
        "\n",
        "// CUDA kernel for matrix multiplication\n",
        "__global__ void matrixMul(const float *A, const float *B, float *C, int width) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    float sum = 0.0;\n",
        "\n",
        "    if (col < width && row < width) {\n",
        "        for (int i = 0; i < width; i++) {\n",
        "            sum += A[row * width + i] * B[i * width + col];\n",
        "        }\n",
        "        C[row * width + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "void printMatrix(const float *matrix, int width, const char* name) {\n",
        "    printf(\"Matrix %s:\\n\", name);\n",
        "    for (int i = 0; i < width; i++) {\n",
        "        for (int j = 0; j < width; j++) {\n",
        "            printf(\"%6.2f \", matrix[i * width + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main() {\n",
        "    int width = WIDTH;\n",
        "    size_t size = width * width * sizeof(float);\n",
        "    float *h_A, *h_B, *h_C;  // host copies of A, B, C\n",
        "    float *d_A, *d_B, *d_C;  // device copies of A, B, C\n",
        "\n",
        "    // Allocate host memory\n",
        "    h_A = (float *)malloc(size);\n",
        "    h_B = (float *)malloc(size);\n",
        "    h_C = (float *)malloc(size);\n",
        "\n",
        "    // Initialize the host matrices\n",
        "    for (int i = 0; i < width * width; ++i) {\n",
        "        h_A[i] = rand()/(float)RAND_MAX;\n",
        "        h_B[i] = rand()/(float)RAND_MAX;\n",
        "    }\n",
        "\n",
        "\n",
        "    // Print matrix A and B\n",
        "    printMatrix(h_A, width, \"A\");\n",
        "    printMatrix(h_B, width, \"B\");\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc(&d_A, size);\n",
        "    cudaMalloc(&d_B, size);\n",
        "    cudaMalloc(&d_C, size);\n",
        "\n",
        "    // Copy host matrices to device\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch the matrix multiplication kernel\n",
        "    dim3 dimBlock(16, 16);\n",
        "    dim3 dimGrid((width + dimBlock.x - 1) / dimBlock.x, (width + dimBlock.y - 1) / dimBlock.y);\n",
        "    matrixMul<<<dimGrid, dimBlock>>>(d_A, d_B, d_C, width);\n",
        "\n",
        "    // Copy the result back to host\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "    // Print result matrix C\n",
        "    printMatrix(h_C, width, \"C\");\n",
        "\n",
        "\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc MatrixM.cu -o matrixmulti"
      ],
      "metadata": {
        "id": "hjNC7R9GmJSO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrixmulti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrdAztSxmQRq",
        "outputId": "01b0b900-c898-4124-dabc-9acbb7887e98"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            "  0.84   0.78   0.91   0.34   0.28   0.48   0.36   0.95   0.64   0.14   0.02   0.14   0.16   0.13   1.00   0.51 \n",
            "  0.61   0.64   0.49   0.29   0.53   0.40   0.28   0.81   0.07   0.53   0.19   0.89   0.06   0.46   0.24   0.90 \n",
            "  0.27   0.38   0.51   0.53   0.44   0.93   0.28   0.64   0.69   0.44   0.83   0.23   0.35   0.96   0.66   0.44 \n",
            "  0.40   0.68   0.48   0.95   0.15   0.64   0.62   0.79   0.45   0.19   0.56   0.17   0.10   0.50   0.98   0.68 \n",
            "  0.75   0.29   0.58   0.15   0.13   0.16   0.07   0.05   0.18   0.80   0.66   0.64   0.09   0.52   0.07   0.46 \n",
            "  0.57   0.05   1.00   0.89   1.00   0.87   0.00   0.59   0.16   0.91   0.36   0.58   0.69   0.53   0.30   0.58 \n",
            "  0.75   0.04   0.83   0.87   0.98   0.90   0.67   0.16   0.89   0.65   0.63   0.70   0.33   0.07   0.22   0.51 \n",
            "  0.28   0.72   0.47   0.94   0.34   0.43   0.34   0.83   0.68   0.48   0.71   0.62   0.41   0.67   0.35   0.61 \n",
            "  0.73   0.74   0.92   0.65   0.53   0.26   0.69   0.11   0.58   0.67   0.78   0.33   0.98   0.83   0.19   0.96 \n",
            "  0.76   0.12   0.38   0.94   0.86   0.79   0.30   0.91   0.50   0.16   0.86   0.46   0.50   0.18   0.73   0.60 \n",
            "  0.84   0.18   0.50   0.14   0.32   0.91   0.84   0.50   0.39   0.61   0.15   0.11   0.36   0.33   0.43   0.58 \n",
            "  0.66   0.49   0.88   0.52   0.56   0.83   0.24   0.73   0.98   0.90   0.41   0.78   0.29   0.87   0.05   0.99 \n",
            "  0.21   0.87   0.10   0.30   0.81   0.05   0.46   0.69   0.12   0.58   0.60   0.30   0.48   0.61   0.62   0.23 \n",
            "  0.07   0.92   0.48   0.83   0.36   0.34   0.66   0.26   0.63   0.31   0.20   0.11   0.78   0.20   0.32   0.23 \n",
            "  0.53   0.56   0.38   0.31   0.26   0.55   0.69   0.70   0.65   0.53   0.62   0.52   0.36   0.80   0.15   0.06 \n",
            "  0.19   0.70   0.00   0.31   0.66   0.18   0.67   0.65   0.89   0.16   0.83   0.90   0.39   0.87   0.74   0.23 \n",
            "\n",
            "Matrix B:\n",
            "  0.39   0.80   0.20   0.77   0.55   0.63   0.51   0.92   0.72   0.61   0.24   0.80   0.40   0.11   0.22   0.84 \n",
            "  0.30   0.52   0.97   0.77   0.77   0.89   0.35   0.92   0.95   0.09   0.66   0.35   0.02   0.06   0.97   0.85 \n",
            "  0.54   0.76   0.67   0.04   0.93   0.72   0.74   0.35   0.17   0.88   0.33   0.89   0.69   0.59   0.86   0.92 \n",
            "  0.81   0.91   0.22   0.92   0.88   0.43   0.28   0.31   0.23   0.28   0.42   0.91   0.13   0.76   0.94   0.38 \n",
            "  0.37   0.23   0.24   0.73   0.79   0.75   0.95   0.52   0.24   0.73   0.97   0.76   0.13   0.08   0.20   0.82 \n",
            "  0.76   0.16   0.20   0.13   0.05   0.07   0.92   0.18   0.39   0.82   0.55   0.45   0.10   0.76   0.99   0.88 \n",
            "  0.63   0.75   0.93   0.83   0.74   0.98   0.50   0.83   0.08   0.25   0.23   0.32   0.23   0.63   0.65   0.97 \n",
            "  0.55   0.11   0.59   0.45   0.85   0.00   0.60   0.23   0.48   0.30   0.18   0.04   0.70   0.64   0.18   0.63 \n",
            "  0.33   0.20   0.68   0.26   0.09   0.88   0.09   0.36   0.59   0.29   0.29   0.19   0.00   0.33   0.44   0.92 \n",
            "  0.70   0.69   0.77   0.92   0.20   0.55   0.90   0.87   0.58   0.27   0.49   0.85   0.29   0.68   0.14   0.49 \n",
            "  0.72   0.22   0.12   0.36   0.93   0.62   0.82   0.33   0.66   0.26   0.07   0.65   0.29   0.09   0.93   0.27 \n",
            "  0.76   0.16   0.63   0.21   0.43   0.39   0.33   0.64   0.34   0.14   0.01   0.77   0.11   0.72   0.45   0.71 \n",
            "  0.47   0.09   0.38   0.66   0.13   0.05   0.78   0.44   0.59   0.53   0.36   0.89   0.17   0.53   0.60   0.83 \n",
            "  0.10   0.17   0.23   0.29   0.88   0.81   0.04   0.78   0.84   0.22   0.61   0.67   0.72   0.40   0.43   0.39 \n",
            "  0.15   0.01   0.38   0.74   0.65   0.92   0.81   0.31   0.01   0.84   0.64   0.40   0.72   0.68   0.03   0.69 \n",
            "  0.62   0.57   0.01   0.26   0.86   0.34   0.88   0.31   0.19   0.50   0.68   0.19   0.71   0.55   0.93   0.93 \n",
            "\n",
            "Matrix C:\n",
            "  3.52   3.19   3.78   3.93   4.92   4.54   4.56   3.76   3.13   3.91   3.33   3.73   3.12   3.65   4.02   6.04 \n",
            "  3.93   3.11   3.34   3.63   4.89   3.86   4.46   4.04   3.25   3.10   3.16   4.03   2.83   3.57   4.06   5.42 \n",
            "  4.26   2.93   3.47   4.00   5.24   4.78   5.04   4.02   3.93   3.88   3.73   4.68   3.12   4.11   4.97   5.82 \n",
            "  4.24   3.46   3.63   4.42   5.66   4.84   4.80   3.92   3.35   3.70   3.59   4.17   3.20   4.23   5.03   5.86 \n",
            "  3.04   2.64   2.42   2.70   3.50   3.30   3.34   3.38   2.86   2.28   2.08   3.74   2.08   2.46   3.14   3.77 \n",
            "  5.03   3.81   3.53   4.56   5.45   4.37   6.04   4.36   3.71   4.60   4.12   6.05   3.23   4.68   5.03   6.37 \n",
            "  5.27   4.14   3.92   4.63   5.24   5.19   5.76   4.53   3.53   4.43   3.75   5.72   2.56   4.46   5.38   6.84 \n",
            "  4.71   3.55   4.02   4.51   5.71   4.83   4.85   4.42   4.14   3.41   3.58   4.91   2.97   4.30   5.35   6.05 \n",
            "  5.06   4.53   4.36   5.20   6.30   5.87   5.85   5.50   4.66   4.25   4.27   6.13   3.39   4.37   6.18   7.20 \n",
            "  4.91   3.38   3.29   4.71   5.72   4.56   5.79   3.97   3.60   4.40   3.75   5.12   3.10   4.32   5.04   6.36 \n",
            "  3.75   3.14   3.24   3.74   4.00   3.94   4.66   3.82   2.95   3.60   3.07   3.85   2.63   3.63   3.88   5.60 \n",
            "  5.38   4.21   4.51   4.56   5.97   5.45   5.83   5.24   4.75   4.36   4.27   5.74   3.55   4.93   5.85   7.40 \n",
            "  3.27   2.48   3.41   4.25   4.73   4.25   4.25   3.98   3.38   2.75   3.29   3.91   2.41   2.95   3.51   4.75 \n",
            "  3.41   2.96   3.56   3.92   3.97   3.97   3.70   3.50   2.98   2.73   2.98   3.80   1.72   3.20   4.32   5.02 \n",
            "  3.90   2.95   3.82   3.86   4.55   4.43   4.06   4.28   3.88   2.91   2.85   4.25   2.45   3.53   4.24   5.35 \n",
            "  3.91   2.49   4.01   4.29   5.26   5.26   4.28   4.46   3.94   3.03   3.35   4.31   2.62   3.64   4.32   5.76 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SehlE20ymUWe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}